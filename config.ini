[TOKENIZER]
max_length = 128
truncate_longer_samples = True
vocab_size = 30522
special_tokens = ['[PAD]', '[UNK]', '[CLS]', '[SEP]', '[MASK]', '<S>', '<T>']

[TRAINING]
model_path = models
evaluation_strategy = steps
overwrite_output_dir = True
num_train_epochs = 100
per_device_train_batch_size = 64
gradient_accumulation_steps = 10
per_device_eval_batch_size = 64
logging_steps = 200000
save_steps = 200000
load_best_model_at_end = True
save_total_limit = 3

